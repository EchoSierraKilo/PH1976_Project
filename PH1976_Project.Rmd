---
title: "PH1976 Project: Predicting Parkinson’s Disease for Patients Using Voice Recording"
author: "Erin S. King, Name, Name, Name, Name"
date: "`r Sys.Date()`"
output: 
  bookdown::gitbook:
    fig_caption: TRUE
    citation_package: biblatex
    biblio-style: apa
    split_bib: false
bibliograpy: packages.bib
---
```{r, include=FALSE}
#Set the working directory and get the data
project_dir = dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(project_dir)
training_set_path = paste(project_dir,'/Project data_2023_p 2/Project data_2023_p/project_training set_p.csv',sep="")
test_set_path = paste(project_dir,'/Project data_2023_p 2/Project data_2023_p/project_test set_p.csv',sep="")
training_data = read.csv(training_set_path)
test_data = read.csv(test_set_path)

# Grab packages if not installed
# Note, if RStudio is having issues w/ tidyverse, run this command in R (not Rstudio):
# install.packages("https://cran.r-project.org/src/contrib/rlang_1.1.0.tar.gz", repos = NULL, type="source")
list.of.packages <- c("tidyverse", "broom", "mosaic","ggplot2", "bookdown", "Boruta","mlbench","caret","randomForest")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dependencies = TRUE)
for(package in list.of.packages){
  require(package, character.only = TRUE)
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::write_bib(list.of.packages, "packages.bib",width = 60)
```

# Introduction
The following analysis in this PH1976 project is to demonstrate the tools examined in this course for data categorization, regression, and prediction. The project's aim is to predict Parkinson’s disease (PD) using the extracted features from the voice recording of patients. For each individual, three recording samples were collected. The [data and corresponding analysis](https://doi.org/10.1016/j.asoc.2018.10.022) is provided by @Sakar in their seminal research on implementing tunable Q-factor wavelet transforms in conjunction with existing data prediction methods. The methods found in this study serve as a roadmap for this project and inform the methods chosen for categorization and prediction.  

## Definition
Parkinson’s disease (PD) is a progressive neurodegenerative disorder. To accurately detect the disease in the
early stage, many telediagnosis and telemonitoring systems have recently been proposed. Since vocal problem is
one of the most important symptoms which can be seen in the earlier stage of PD patients, vocal disorders-
based systems become popular in PD diagnosis and monitoring. In these systems, various speech signal
processing algorithms have been used to extract clinically useful information for PD assessment, and the
calculated features are fed to different learning algorithms to make reliable decisions. PD telemedicine studies
showed that the choice of extracted features and learning algorithms directly influences the accuracy and
reliability of distinguishing PD patients.  

## Data
In this study, Sakar et. al collected the voice recordings of 252 subjects including PD patients and healthy individuals. They gathered three recording samples from each subject and extracted seven feature subsets from the recording samples. The feature subsets were baseline features, intensity-based features, bandwidth and formant features, vocal fold features, Mel Frequency Cepstral Coefficients (MFCC), wavelet transform based features (WT) and tunable Q-factor wavelet transform based features (TQWT).  

## Study Design
Need to fill this out based on our own analysis.  

## Study Population
The dataset includes PD patients with age ranging from 33 to 87 (65.1 ± 10.9) and healthy individuals with age
ranging from 41 to 82 (61.1 ± 8.9). Each patient has three voice recording samples, with 7 aforementioned
feature subsets. Each feature subset contains several features.  

# Dataset
## Preprocessing and Standardization  
Instead of using the LOOCV method outlined in the paper, we have decided to break the training dataset into a 90/10 split, where 90% of the data will be used to train, and 10% of the data will be used to check accuracy of the model. The best model for each subset will be selected based on these results.  

```{r,include=FALSE}
# Load caret package if not already loaded
if (!"caret" %in% rownames(installed.packages())) {
  install.packages("caret")
}
library(caret)

# Set seed for reproducibility
set.seed(123)

#Break the training data into feature subsets
train_df.baseline = data.frame(training_data[c(1:24)])
train_df.intensity = data.frame(training_data[c(1:3,25:27)])
train_df.formant = data.frame(training_data[c(1:3,28:35)])
train_df.vff = data.frame(training_data[c(1:3,36:57)])
train_df.mfcc = data.frame(training_data[c(1:3,58:141)])
train_df.wt = data.frame(training_data[c(1:3,142:323)])
train_df.tqwt = data.frame(training_data[c(1:3,324:755)])

#Partition training data
train_indices <- createDataPartition(train_df.baseline$class, p = 0.9, list = FALSE)

# Split train_df data
train_df.baseline_train <- train_df.baseline[train_indices, ]
train_df.baseline_test <- train_df.baseline[-train_indices, ]

train_df.intensity_train <- train_df.intensity[train_indices, ]
train_df.intensity_test <- train_df.intensity[-train_indices, ]

train_df.formant_train <- train_df.formant[train_indices, ]
train_df.formant_test <- train_df.formant[-train_indices, ]

train_df.vff_train <- train_df.vff[train_indices, ]
train_df.vff_test <- train_df.vff[-train_indices, ]

train_df.mfcc_train <- train_df.mfcc[train_indices, ]
train_df.mfcc_test <- train_df.mfcc[-train_indices, ]

train_df.wt_train <- train_df.wt[train_indices, ]
train_df.wt_test <- train_df.wt[-train_indices, ]

train_df.tqwt_train <- train_df.tqwt[train_indices, ]
train_df.tqwt_test <- train_df.tqwt[-train_indices, ]

#Break the test data into feature subsets
test_df.baseline = data.frame(test_data[c(1:24)])
test_df.intensity = data.frame(test_data[c(1:3,25:27)])
test_df.formant = data.frame(test_data[c(1:3,28:35)])
test_df.vff = data.frame(test_data[c(1:3,36:57)])
test_df.mfcc = data.frame(test_data[c(1:3,58:141)])
test_df.wt = data.frame(test_data[c(1:3,142:323)])
test_df.tqwt = data.frame(test_data[c(1:3,324:755)])
```
For ease of analysis, the ensemble data set (both training and test) are broken into the following sub feature categories:  

* Baseline Features
* Time Frequency Features 
    + Intensity based
    + Formant and Bandwidth based
* Vocal Fold Features
* Mel Frequency Cepstral Coefficients (MFCC)
* Wavelet Transform-based Features
* Tunable Q-Factor Wavelet Transform-based Features (TQWT)  

```{r, include=FALSE}

# Standardizing the data for cross-comparison

require(tidyverse)
require(broom)
require(mosaic)

# Standardizing the data for cross-comparison
# Training and Test Data
subset_names <- c("baseline", "intensity", "formant", "vff", "mfcc", "wt", "tqwt")

# Standardize function
standardize_data <- function(train_df, test_df) {
  for (ii in 4:length(train_df)) {
    mean_val <- mean(train_df[, ii], na.rm = TRUE)
    std_val <- sd(train_df[, ii], na.rm = TRUE)
    
    train_df[, ii] <- (train_df[, ii] - mean_val) / std_val
    
    if (!is.null(test_df)) {
      test_df[, ii] <- (test_df[, ii] - mean_val) / std_val
    }
  }
  return(list(train_df, test_df))
}

# Standardize train and train_test datasets
for (i in subset_names) {
  # Standardize train_df and train_test_df
  standardized_data <- standardize_data(get(paste0("train_df.", i, "_train")), get(paste0("train_df.", i, "_test")))
  assign(paste0("train_df_std.", i), standardized_data[[1]])
  assign(paste0("train_test_df_std.", i), standardized_data[[2]])
}

# Standardize test dataset
for (i in subset_names) {
  # Standardize test_df
  standardized_data <- standardize_data(get(paste0("train_df.", i, "_train")), get(paste0("test_df.", i)))
  assign(paste0("test_df_std.", i), standardized_data[[2]])
}
```

To start, data from all sub features are standardized such that each feature has zero mean and unit variance. This was accomplished using the [@tidyverse], [@broom], and [@mosaic] packages in RStudio. The histograms below shows an example transformation of the original training data set to the standardized form from the **Baseline**, **Intensity**, and **Formant** sub features. This allows all data comparisons to be made equivalently. To ensure that training and test data are all benchmarked equivalently, mean and standard deviation is calculated using the training data, and is applied to standardize both the training and test data. This way, no information leakage will occur and the models will be provided standardized data that is unbiased.  

```{r, include = FALSE}

plot_subfeatures <- function(subfeature_name, train_df, train_df_std) {
  # Pre-standardization
  cat(paste0("\\begin{figure}[h]\\centering\n"))
  cat(paste0("\\caption{", subfeature_name, " Sub-feature: Pre-standardization}\n"))
  
  par(mfrow = c(2, 4))
  for (ii in 3:ncol(train_df)) {
    hist(train_df[, ii], main = "", xlab = colnames(train_df)[ii])
  }
  cat("\\end{figure}\n")
  
  # Post-standardization
  cat(paste0("\\begin{figure}[h]\\centering\n"))
  cat(paste0("\\caption{", subfeature_name, " Sub-feature: Post-standardization}\n"))
  
  par(mfrow = c(2, 4))
  for (ii in 3:ncol(train_df_std)) {
    hist(train_df_std[, ii], main = "", xlab = colnames(train_df_std)[ii])
  }
  cat("\\end{figure}\n")
}

```

```{r, echo=FALSE, results='asis'}

selected_subsets <- c("baseline", "intensity", "formant")
# Generate plots using function
for (selected_subset in selected_subsets) {
  train_df_subset <- get(paste0("train_df.", selected_subset, "_train"))
  train_df_std_subset <- get(paste0("train_df_std.", selected_subset))
  
  plot_subfeatures(selected_subset, train_df_subset, train_df_std_subset)
}
```
  
## Feature Selection
Per the **Sakar et al** paper, minimum redundancy-maximum relevance based filter feature selection methods are ideal for determining effective features. The advantage of this is two-fold:
1. It reduces the high dimensionality of the data set.
2. It maximizes the joint dependency of the data set.
This strategy is used frequenty in machine learning and regression applications, and as such, will be used in this analysis. The **Boruta** package in RStudio will be used for this purpose, and utilizes Random Forest to perform a top-down search on the corresponding data frame to determine relevant features.     



```{r boruta, include = FALSE}
require(Boruta)
require(mlbench)
require(caret)
require(randomForest)
#Find important features in training data
set.seed(123)
boruta.baseline = Boruta(class ~ ., data = train_df.baseline.std, doTrace = 2)
boruta.formant = Boruta(class ~., data = train_df.formant.std, doTrace = 2,maxRuns = 500)
boruta.intensity = Boruta(class ~., data = train_df.intensity.std, doTrace = 2,maxRuns = 500)
boruta.mfcc = Boruta(class ~., data = train_df.mfcc.std, doTrace = 2,maxRuns = 500)
boruta.tqwt = Boruta(class ~., data = train_df.tqwt.std, doTrace = 2,maxRuns = 500)
boruta.vff = Boruta(class ~., data = train_df.vff.std, doTrace = 2,maxRuns = 500)
boruta.wt = Boruta(class ~., data = train_df.wt.std, doTrace = 2,maxRuns = 500)
```
   
mRMR analysis yields the following results:  

* Baseline
```{r, echo=FALSE}
print(boruta.baseline)
```
* Formant
```{r, echo=FALSE}
print(boruta.formant)
```
* Intensity
```{r, echo=FALSE}
print(boruta.intensity)
```
* MFCC
```{r, echo=FALSE}
print(boruta.mfcc)
```
* TQWT
```{r, echo=FALSE}
print(boruta.tqwt)
```
* VFF
```{r, echo=FALSE}
print(boruta.vff)
```
* WT
```{r, echo=FALSE}
print(boruta.wt)
```
 
 
```{r boruta.baseline, echo=FALSE, fig.align='center',fig.cap="\\label{fig:boruta.baseline}Boruta plot for Baseline features"}
#Generate Boruta plots
plot(boruta.baseline, las = 2,main="Baseline mRMR")
```
```{r boruta.formant, echo=FALSE, fig.align='center',fig.cap="\\label{fig:boruta.formant}Boruta plot for Formant features"}
plot(boruta.formant, las = 2,main = "Formant mRMR")
```
```{r boruta.intensity, echo=FALSE, fig.align='center',fig.cap="\\label{fig:boruta.intensity}Boruta plot for Intensity features"}
plot(boruta.intensity, las = 2, main="Intensity mRMR")
```
```{r boruta.mfcc, echo=FALSE, , fig.align='center',fig.cap="\\label{fig:boruta.mfcc}Boruta plot for MFCC features"}
plot(boruta.mfcc, las = 2, main="MFCC mRMR")
```
```{r boruta.tqwt, echo=FALSE, , fig.align='center',fig.cap="\\label{fig:boruta.tqwt}Boruta plot for TQWT features"}
plot(boruta.tqwt, las = 2, main="TQWT mRMR")
```
```{r boruta.vff, echo=FALSE, , fig.align='center',fig.cap="\\label{fig:boruta.vff}Boruta plot for VFF features"}
plot(boruta.vff, las = 2, main="VFF mRMR")
```
```{r boruta.wt, echo=FALSE, fig.align='center',fig.cap="\\label{fig:boruta.wt}Boruta plot for WT features"}
plot(boruta.wt, las = 2, main="WT mRMR")
```
  
Following this initial assessment, chosen variables are selected for regression by using **getNonRejectedFormula()**. This collapses any variables left as **Tentative** factors into either 
```{r, include=FALSE}
#Force "Tentative" values
chosen.baseline = getNonRejectedFormula(TentativeRoughFix(boruta.baseline))
chosen.formant = getNonRejectedFormula(TentativeRoughFix(boruta.formant))
chosen.intensity = getNonRejectedFormula(TentativeRoughFix(boruta.intensity))
chosen.mfcc = getNonRejectedFormula(TentativeRoughFix(boruta.mfcc))
chosen.tqwt = getNonRejectedFormula(TentativeRoughFix(boruta.tqwt))
chosen.vff = getNonRejectedFormula(TentativeRoughFix(boruta.vff))
chosen.wt = getNonRejectedFormula(TentativeRoughFix(boruta.wt))
<<<<<<< Updated upstream
```
  
## Baseline Features

For baseline features, both Random Forest and Logistic Regression are investigated.
```{r, include = FALSE}
library(boot)
library(randomForest)
#Logistic
logit.baseline = glm(formula = chosen.baseline, family="binomial",data=train_df.baseline.std)
err.logit.baseline = cv.glm(train_df.baseline,logit.baseline)$delta[1]

#Random Forest
rf.baseline = randomForest(chosen.baseline,data=train_df.baseline.std,importance=TRUE)
```

```{r,echo=FALSE}
importance(rf.baseline)
varImpPlot(rf.baseline)
err.rf.baseline = last(rf.baseline$mse)
```
  
```{r baseline.comparison, echo=FALSE, fig.align='center',fig.cap="\\label{fig:baseline.comparison}MSE for Baseline comparison: Logistic Regression vs. Random Forest"}
barplot(c(err.logit.baseline,err.rf.baseline))
```
=======

```

```{r}
# Separate the predictors and the target variable in both datasets
train_baseline_class <- train_df.baseline[, 1]
train_baseline_data <- train_df.baseline[, -1]
test_baseline_class <- test_df.baseline[, 1]
test_baseline_data <- test_df.baseline[, -1]

train_intensity_class <- train_df.intensity[, 1]
train_intensity_data <- train_df.intensity[, -1]
test_intensity_class <- test_df.intensity[, 1]
test_intensity_data <- test_df.intensity[, -1]

train_formant_class <- train_df.formant[, 1]
train_formant_data <- train_df.formant[, -1]
test_formant_class <- test_df.formant[, 1]
test_formant_data <- test_df.formant[, -1]

train_vff_class <- train_df.vff[, 1]
train_vff_data <- train_df.vff[, -1]
test_vff_class <- test_df.vff[, 1]
test_vff_data <- test_df.vff[, -1]

train_mfcc_class <- train_df.mfcc[, 1]
train_mfcc_data <- train_df.mfcc[, -1]
test_mfcc_class <- test_df.mfcc[, 1]
test_mfcc_data <- test_df.mfcc[, -1]

train_wt_class <- train_df.wt[, 1]
train_wt_data <- train_df.wt[, -1]
test_wt_class <- test_df.wt[, 1]
test_wt_data <- test_df.wt[, -1]

train_tqwt_class <- train_df.tqwt[, 1]
train_tqwt_data <- train_df.tqwt[, -1]
test_tqwt_class <- test_df.tqwt[, 1]
test_tqwt_data <- test_df.tqwt[, -1]

```

```{r}
# Baseline
#SVM
# Create, train, and test SVM with Linear kernel for accuracy
accuracy_linear <- 0
svm_linear <- svm(train_baseline_data, train_baseline_class, kernel = "linear")
predictions_linear <- predict(svm_linear, test_baseline_data)
accuracy_linear <- sum(predictions_linear == test_baseline_class) / length(test_baseline_class)

# Create, train, and test SVM with RBF kernel for accuracy
accuracy_rbf <- 0
svm_rbf <- svm(train_baseline_data, train_baseline_class, kernel = "radial")
predictions_rbf <- predict(svm_rbf, test_baseline_data)
class_labels <- ifelse(predictions_rbf > 0.5, 1, 0)
accuracy_rbf <- sum(class_labels == test_baseline_class) / length(test_baseline_class)


#Multilayer Perceptron
install.packages("neuralnet")
library(neuralnet)

# train_df.baseline$class <- as.logical(train_df.baseline$class)
nn.2 <-nnet(class ~ .,data = train_df.baseline, size = 2,decay = 1e-5,maxit = 100)
predicted.class <- predict(nn.2, test_df.baseline, type = "class")
comparison <- data.frame(actual = test.data$class, predicted = predicted.class)

  # How did we do?
  table(comparison)


# Naive Bayes

formula <- class ~ .
nb_model <- naiveBayes(formula, data = train_df.baseline)
predictions <- predict(nb_model, newdata = test_df.baseline)
accuracy_nb <- sum(predictions == test_baseline_class) / length(test_baseline_class)


# Logistic Regression

model <- glm(class ~ ., data = train_df.baseline, family = binomial)
predictions <- predict(model, newdata = test_set, type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
actual_classes <- test_df.baseline$class
accuracy <- sum(predicted_classes == actual_classes) / length(actual_classes)


# Random Forest
rf_model <- randomForest(class ~ .,data = train_df.baseline, ntree = 500, mtry = sqrt(ncol(train_df.baseline)), importance = TRUE)
rf_pred <- predict(rf_model, newdata = test_df.baseline)
accuracy <- sum(rf_pred == test_df.baseline$class) / length(test_df.baseline$class)


# KNN

formula <- class ~ .
knn_params <- expand.grid(k = 10)
knn_model <- train( formula = formula, data = train_df.baseline, method = "knn", tuneGrid = knn_params, trControl = trainControl(method = "cv"))
test_predictions <- predict(knn_model, newdata = test_baseline_data)

```

```{r}
# Intensity
# Create, train, and test SVM with Linear kernel for accuracy
accuracy_linear_int <- 0
svm_linear_int <- svm(train_intensity_data, train_intensity_class, kernel = "linear")
predictions_linear_int <- predict(svm_linear_int, test_intensity_data)
accuracy_linear_int <- sum(predictions_linear_int == test_intensity_class) / length(test_intensity_class)

# Create, train, and test SVM with RBF kernel for accuracy
accuracy_rbf_int <- 0
svm_rbf_int <- svm(train_intensity_data, train_intensity_class, kernel = "radial")
predictions_rbf_int <- predict(svm_rbf_int, test_intensity_data)
class_labels_int <- ifelse(predictions_rbf_int > 0.5, 1, 0)
accuracy_rbf_int <- sum(class_labels_int == test_intensity_class) / length(test_intensity_class)


# Multilayer Perceptron
install.packages("neuralnet")
library(neuralnet)

# train_df.intensity$class <- as.logical(train_df.intensity$class)
nn.2_int <-nnet(class ~ .,data = train_df.intensity, size = 2,decay = 1e-5,maxit = 100)
predicted.class_int <- predict(nn.2_int, test_df.intensity, type = "class")
comparison_int <- data.frame(actual = test.data$class, predicted = predicted.class_int)

# How did we do?
table(comparison_int)


# Naive Bayes

formula_int <- class ~ .
nb_model_int <- naiveBayes(formula_int, data = train_df.intensity)
predictions_int <- predict(nb_model_int, newdata = test_df.intensity)
accuracy_nb_int <- sum(predictions_int == test_intensity_class) / length(test_intensity_class)


# Logistic Regression

model_int <- glm(class ~ ., data = train_df.intensity, family = binomial)
predictions_int <- predict(model_int, newdata = test_set, type = "response")
predicted_classes_int <- ifelse(predictions_int > 0.5, 1, 0)
actual_classes_int <- test_df.intensity$class
accuracy_int <- sum(predicted_classes_int == actual_classes_int) / length(actual_classes_int)


# Random Forest
rf_model_int <- randomForest(class ~ .,data = train_df.intensity, ntree = 500, mtry = sqrt(ncol(train_df.intensity)), importance = TRUE)
rf_pred_int <- predict(rf_model_int, newdata = test_df.intensity)
accuracy_int <- sum(rf_pred_int == test_df.intensity$class) / length(test_df.intensity$class)

#KNN
formula_formant <- class ~ intensity_formant
knn_params_formant <- expand.grid(k = 10)
knn_model_formant <- train( formula = formula_formant, data = train_df_intensity, method = "knn", tuneGrid = knn_params_formant, trControl = trainControl(method = "cv"))
test_predictions_formant <- predict(knn_model_formant, newdata = test_intensity_formant_data)

```

```{r}
#Formant
# Create, train, and test SVM with Linear kernel for accuracy
accuracy_linear_formant <- 0
svm_linear_formant <- svm(train_formant_data, train_formant_class, kernel = "linear")
predictions_linear_formant <- predict(svm_linear_formant, test_formant_data)
accuracy_linear_formant <- sum(predictions_linear_formant == test_formant_class) / length(test_formant_class)

# Create, train, and test SVM with RBF kernel for accuracy
accuracy_rbf_formant <- 0
svm_rbf_formant <- svm(train_formant_data, train_formant_class, kernel = "radial")
predictions_rbf_formant <- predict(svm_rbf_formant, test_formant_data)
class_labels_formant <- ifelse(predictions_rbf_formant > 0.5, 1, 0)
accuracy_rbf_formant <- sum(class_labels_formant == test_formant_class) / length(test_formant_class)


#Multilayer Perceptron
install.packages("neuralnet")
library(neuralnet)

# train_df.formant$class <- as.logical(train_df.formant$class)
nn.2_formant <-nnet(class ~ .,data = train_df.formant, size = 2,decay = 1e-5,maxit = 100)
predicted.class_formant <- predict(nn.2_formant, test_df.formant, type = "class")
comparison_formant <- data.frame(actual = test_data$formant, predicted = predicted.class_formant)

# How did we do?
table(comparison_formant)


# Naive Bayes

formula_formant <- class ~ .
nb_model_formant <- naiveBayes(formula_formant, data = train_df.formant)
predictions_formant <- predict(nb_model_formant, newdata = test_df.formant)
accuracy_nb_formant <- sum(predictions_formant == test_formant_class) / length(test_formant_class)


# Logistic Regression

model_formant <- glm(class ~ ., data = train_df.formant, family = binomial)
predictions_formant <- predict(model_formant, newdata = test_set_formant, type = "response")
predicted_classes_formant <- ifelse(predictions_formant > 0.5, 1, 0)
actual_classes_formant <- test_df.formant$class
accuracy_formant <- sum(predicted_classes_formant == actual_classes_formant) / length(actual_classes_formant)


# Random Forest
rf_model_formant <- randomForest(class ~ .,data = train_df.formant, ntree = 500, mtry = sqrt(ncol(train_df.formant)), importance = TRUE)
rf_pred_formant <- predict(rf_model_formant, newdata = test_df.formant)
accuracy_formant <- sum(rf_pred_formant == test_df.formant$class) / length(test_df.formant$class)

#KNN
formula_formant <- class_formant ~ .
knn_params_formant <- expand.grid(k_formant = 10)
knn_model_formant <- train(formula = formula_formant, data = train_df.formant, method = "knn", tuneGrid = knn_params_formant, trControl = trainControl(method = "cv"))
test_predictions_formant <- predict(knn_model_formant, newdata = test_formant_data)

```

```{r}
#VFF
# Create, train, and test SVM with Linear kernel for accuracy
accuracy_linear_vff <- 0
svm_linear_vff <- svm(train_VFF_data, train_VFF_class, kernel = "linear")
predictions_linear_vff <- predict(svm_linear_vff, test_VFF_data)
accuracy_linear_vff <- sum(predictions_linear_vff == test_VFF_class) / length(test_VFF_class)

# Create, train, and test SVM with RBF kernel for accuracy
accuracy_rbf_vff <- 0
svm_rbf_vff <- svm(train_VFF_data, train_VFF_class, kernel = "radial")
predictions_rbf_vff <- predict(svm_rbf_vff, test_VFF_data)
class_labels_vff <- ifelse(predictions_rbf_vff > 0.5, 1, 0)
accuracy_rbf_vff <- sum(class_labels_vff == test_VFF_class) / length(test_VFF_class)


#Multilayer Perceptron
install.packages("neuralnet")
library(neuralnet)

# train_df.VFF$class <- as.logical(train_df.VFF$class)
nn.2_vff <-nnet(class ~ .,data = train_df.VFF, size = 2,decay = 1e-5,maxit = 100)
predicted.class_vff <- predict(nn.2_vff, test_df.VFF, type = "class")
comparison_vff <- data.frame(actual = test.data_VFF$class, predicted = predicted.class_vff)

# How did we do?
table(comparison_vff)


# Naive Bayes

formula_vff <- class ~ .
nb_model_vff <- naiveBayes(formula_vff, data = train_df.VFF)
predictions_vff <- predict(nb_model_vff, newdata = test_df.VFF)
accuracy_nb_vff <- sum(predictions_vff == test_VFF_class) / length(test_VFF_class)


# Logistic Regression

model_vff <- glm(class ~ ., data = train_df.VFF, family = binomial)
predictions_vff <- predict(model_vff, newdata = test_set_VFF, type = "response")
predicted_classes_vff <- ifelse(predictions_vff > 0.5, 1, 0)
actual_classes_vff <- test_df.VFF$class
accuracy_vff <- sum(predicted_classes_vff == actual_classes_vff) / length(actual_classes_vff)


# Random Forest
rf_model_vff <- randomForest(class ~ .,data = train_df.VFF, ntree = 500, mtry = sqrt(ncol(train_df.VFF)), importance = TRUE)
rf_pred_vff <- predict(rf_model_vff, newdata = test_df.VFF)
accuracy_vff <- sum(rf_pred_vff == test_df.VFF$class) / length(test_df.VFF$class)

# KNN
formula_vff <- class_vff ~ .
knn_params_vff <- expand.grid(k_vff = 10)
knn_model_vff <- train(formula = formula_vff, data = train_df.vff, method = "knn", tuneGrid = knn_params_vff, trControl = trainControl(method = "cv"))
test_predictions_vff <- predict(knn_model_vff, newdata = test_vff_data)


```

```{r}
#MFCC
# Create, train, and test SVM with Linear kernel for accuracy
accuracy_linear_mfcc <- 0
svm_linear_mfcc <- svm(train_MFCC_data, train_MFCC_class, kernel = "linear")
predictions_linear_mfcc <- predict(svm_linear_mfcc, test_MFCC_data)
accuracy_linear_mfcc <- sum(predictions_linear_mfcc == test_MFCC_class) / length(test_MFCC_class)

# Create, train, and test SVM with RBF kernel for accuracy
accuracy_rbf_mfcc <- 0
svm_rbf_mfcc <- svm(train_MFCC_data, train_MFCC_class, kernel = "radial")
predictions_rbf_mfcc <- predict(svm_rbf_mfcc, test_MFCC_data)
class_labels_mfcc <- ifelse(predictions_rbf_mfcc > 0.5, 1, 0)
accuracy_rbf_mfcc <- sum(class_labels_mfcc == test_MFCC_class) / length(test_MFCC_class)


#Multilayer Perceptron
install.packages("neuralnet")
library(neuralnet)

# train_df.MFCC$class <- as.logical(train_df.MFCC$class)
nn.2_mfcc <-nnet(class ~ .,data = train_df.MFCC, size = 2,decay = 1e-5,maxit = 100)
predicted.class_mfcc <- predict(nn.2_mfcc, test_df.MFCC, type = "class")
comparison_mfcc <- data.frame(actual = test_data$MFCC, predicted = predicted.class_mfcc)

# How did we do?
table(comparison_mfcc)


# Naive Bayes

formula_mfcc <- class ~ .
nb_model_mfcc <- naiveBayes(formula_mfcc, data = train_df.MFCC)
predictions_mfcc <- predict(nb_model_mfcc, newdata = test_df.MFCC)
accuracy_nb_mfcc <- sum(predictions_mfcc == test_MFCC_class) / length(test_MFCC_class)


# Logistic Regression

model_mfcc <- glm(class ~ ., data = train_df.MFCC, family = binomial)
predictions_mfcc <- predict(model_mfcc, newdata = test_set, type = "response")
predicted_classes_mfcc <- ifelse(predictions_mfcc > 0.5, 1, 0)
actual_classes_mfcc <- test_df.MFCC$class
accuracy_mfcc <- sum(predicted_classes_mfcc == actual_classes_mfcc) / length(actual_classes_mfcc)


# Random Forest
rf_model_mfcc <- randomForest(class ~ .,data = train_df.MFCC, ntree = 500, mtry = sqrt(ncol(train_df.MFCC)), importance = TRUE)
rf_pred_mfcc <- predict(rf_model_mfcc, newdata = test_df.MFCC)
accuracy_mfcc <- sum(rf_pred_mfcc == test_df.MFCC$class) / length(test_df.MFCC$class)

#KFF
formula_mcff <- class_mcff ~ .
knn_params_mcff <- expand.grid(k_mcff = 10)
knn_model_mcff <- train( formula = formula_mcff, data = train_df.mcff, method = "knn", tuneGrid = knn_params_mcff, trControl = trainControl(method = "cv"))
test_predictions_mcff <- predict(knn_model_mcff, newdata = test_mcff_data)

```

```{r}
#WT
# Create, train, and test SVM with Linear kernel for accuracy
accuracy_linear_wt <- 0
svm_linear_wt <- svm(train_WT_data, train_WT_class, kernel = "linear")
predictions_linear_wt <- predict(svm_linear_wt, test_WT_data)
accuracy_linear_wt <- sum(predictions_linear_wt == test_WT_class) / length(test_WT_class)

# Create, train, and test SVM with RBF kernel for accuracy
accuracy_rbf_wt <- 0
svm_rbf_wt <- svm(train_WT_data, train_WT_class, kernel = "radial")
predictions_rbf_wt <- predict(svm_rbf_wt, test_WT_data)
class_labels_wt <- ifelse(predictions_rbf_wt > 0.5, 1, 0)
accuracy_rbf_wt <- sum(class_labels_wt == test_WT_class) / length(test_WT_class)


#Multilayer Perceptron
install.packages("neuralnet")
library(neuralnet)

# train_df.WT$class <- as.logical(train_df.WT$class)
nn.2_wt <-nnet(class ~ .,data = train_df.WT, size = 2,decay = 1e-5,maxit = 100)
predicted.class_wt <- predict(nn.2_wt, test_df.WT, type = "class")
comparison_wt <- data.frame(actual = test_data$WT, predicted = predicted.class_wt)

# How did we do?
table(comparison_wt)


# Naive Bayes

formula_wt <- class ~ .
nb_model_wt <- naiveBayes(formula_wt, data = train_df.WT)
predictions_wt <- predict(nb_model_wt, newdata = test_df.WT)
accuracy_nb_wt <- sum(predictions_wt == test_WT_class) / length(test_WT_class)


# Logistic Regression

model_wt <- glm(class ~ ., data = train_df.WT, family = binomial)
predictions_wt <- predict(model_wt, newdata = test_set_wt, type = "response")
predicted_classes_wt <- ifelse(predictions_wt > 0.5, 1, 0)
actual_classes_wt <- test_df.WT$class
accuracy_wt <- sum(predicted_classes_wt == actual_classes_wt) / length(actual_classes_wt)


# Random Forest
rf_model_wt <- randomForest(class ~ .,data = train_df.WT, ntree = 500, mtry = sqrt(ncol(train_df.WT)), importance = TRUE)
rf_pred_wt <- predict(rf_model_wt, newdata = test_df.WT)
accuracy_wt <- sum(rf_pred_wt == test_df.WT$class) / length(test_df.WT$class)

# KNN
formula_wt <- class_wt ~ .
knn_params_wt <- expand.grid(k_wt = 10)
knn_model_wt <- train(formula = formula_wt, data = train_df_wt, method = "knn", tuneGrid = knn_params_wt, trControl = trainControl(method = "cv"))
test_predictions_wt <- predict(knn_model_wt, newdata = test_wt_data)

```

```{r}
#TQWT
# Create, train, and test SVM with Linear kernel for accuracy
accuracy_linear_tqwt <- 0
svm_linear_tqwt <- svm(train_TQWT_data, train_TQWT_class, kernel = "linear")
predictions_linear_tqwt <- predict(svm_linear_tqwt, test_TQWT_data)
accuracy_linear_tqwt <- sum(predictions_linear_tqwt == test_TQWT_class) / length(test_TQWT_class)

# Create, train, and test SVM with RBF kernel for accuracy
accuracy_rbf_tqwt <- 0
svm_rbf_tqwt <- svm(train_TQWT_data, train_TQWT_class, kernel = "radial")
predictions_rbf_tqwt <- predict(svm_rbf_tqwt, test_TQWT_data)
class_labels_tqwt <- ifelse(predictions_rbf_tqwt > 0.5, 1, 0)
accuracy_rbf_tqwt <- sum(class_labels_tqwt == test_TQWT_class) / length(test_TQWT_class)

#Multilayer Perceptron
install.packages("neuralnet")
library(neuralnet)

# train_df.TQWT$class <- as.logical(train_df.TQWT$class)
nn.2_tqwt <- nnet(class ~ ., data = train_df_TQWT, size = 2, decay = 1e-5, maxit = 100)
predicted_class_tqwt <- predict(nn.2_tqwt, test_df_TQWT, type = "class")
comparison_tqwt <- data.frame(actual = test_TQWT_class, predicted = predicted_class_tqwt)

# How did we do?
table(comparison_tqwt)

# Naive Bayes
formula_tqwt <- class ~ .
nb_model_tqwt <- naiveBayes(formula_tqwt, data = train_df_TQWT)
predictions_tqwt <- predict(nb_model_tqwt, newdata = test_df_TQWT)
accuracy_nb_tqwt <- sum(predictions_tqwt == test_TQWT_class) / length(test_TQWT_class)

# Logistic Regression
model_tqwt <- glm(class ~ ., data = train_df_TQWT, family = binomial)
predictions_tqwt <- predict(model_tqwt, newdata = test_set_TQWT, type = "response")
predicted_classes_tqwt <- ifelse(predictions_tqwt > 0.5, 1, 0)
actual_classes_tqwt <- test_df_TQWT$class
accuracy_tqwt <- sum(predicted_classes_tqwt == actual_classes_tqwt) / length(actual_classes_tqwt)

# Random Forest
rf_model_tqwt <- randomForest(class ~ ., data = train_df_TQWT, ntree = 500, mtry = sqrt(ncol(train_df_TQWT)), importance = TRUE)
rf_pred_tqwt <- predict(rf_model_tqwt, newdata = test_df_TQWT)
accuracy_tqwt <- sum(rf_pred_tqwt == test_df_TQWT$class) / length(test_df_TQWT$class)

#KNN
formula_tqwt <- class_tqwt ~ .
knn_params_tqwt <- expand.grid(k = 10)
knn_model_tqwt <- train(formula = formula_tqwt, data = train_df.TQWT, method = "knn", tuneGrid = knn_params_tqwt, trControl = trainControl(method = "cv"))
test_predictions_tqwt <- predict(knn_model_tqwt, newdata = test_TQWT_data)

```


>>>>>>> Stashed changes
